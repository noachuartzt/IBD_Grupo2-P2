{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnull\n",
    "import os\n",
    "\n",
    "# Crear la sesión Spark\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"lengthwordCount\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "# Preprocesado\n",
    "# =====================================================================\n",
    "# Leer todos los archivos JSON del corpus (uniéndolos en un mismo DataFrame)\n",
    "directory = './datain'\n",
    "\n",
    "# Lista de archivos en el directorio\n",
    "corpus = os.listdir(directory)\n",
    "\n",
    "json_df = spark.read.json([\"datain/\" + file for file in corpus])\n",
    "\n",
    "# Eliminar registros con abstract nulo\n",
    "json_df_filtered = json_df.filter(col('abstract').isNotNull())\n",
    "\n",
    "# MAP-REDUCE\n",
    "# =====================================================================\n",
    "# Seleccionar el campo abstract y convertirlo en un RDD\n",
    "words_rdd = json_df_filtered.select(\"abstract\").rdd.flatMap(lambda x: x[0].split())\n",
    "\n",
    "# Longitud de las palabras a contar\n",
    "n = 6\n",
    "\n",
    "# Filtrar palabras con longitud n y contar su frecuencia\n",
    "counts_rdd = words_rdd.filter(lambda word: len(word) == n).map(lambda word: (n, 1)).reduceByKey(lambda a, b: a + b)\n",
    "# counts_rdd = words_rdd.filter(lambda word: len(word) == n).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) # si se quieren saber cuales son las palabras de longitud n\n",
    "\n",
    "# Mostrar resultados\n",
    "print(counts_rdd.collect())\n",
    "\n",
    "# Cerrar la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
