paperId,title,abstract,year,publicationDate,authorId,authorName
27b9f7b653b79b65d78db0ceb1688a270316331a,Depicting Vocabulary Summaries with Devos,"Communicating ontologies to potential users is still a difficult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams together with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on different strategies for summarizing the ontology.",2023,2023-04-30,"2691947,2056266963,70053552,1403446235","Ahmad Alobaid,Jhon Toledo,Óscar Corcho,M. Poveda-Villalón"
2ca14fe14f0bd2f1363f3b735e788d12c3f9f332,"Comparison between Expert Systems, Machine Learning, and Big Data: An Overview","Today, the science of artificial intelligence has become one of the most important sciences in creating intelligent computer programs that simulate the human mind. The goal of artificial intelligence in the medical field is to assist doctors and health care workers in diagnosing diseases and clinical treatment, reducing the rate of medical error, and saving lives of citizens. The main and widely used technologies are expert systems, machine learning and big data. In the article, a brief overview of the three mentioned techniques will be provided to make it easier for readers to understand these techniques and their importance.",2022,2022-03-01,"1394846990,2149322954,21366673,2061630849,2156915219","Maad M. Mijwil,Dhamyaa Salim Mutar,Y. Filali,Karan Aggarwal,Humam Al-Shahwani"
3a070f7534119762099e7f3b373c54bdeddddd36,A SAREF Extension for Semantic Interoperability in the Industry and Manufacturing Domain,,2018,2018-10-26,"1795459,69903156,50788142,47922982,2075047311,2107272840","L. Daniele,M. Punter,C. Brewster,Raúl García Castro,María Poveda,A. Fernandez"
3f71378310cccfe42951940a868c4dfa96f91bb9,"Polypharmacy and drug-drug interactions in HIV-infected subjects in the region of Madrid, Spain: a population-based study.","BACKGROUND
Drug-drug interactions (DDIs) involving antiretrovirals (ARVs) tend to cause harm if unrecognized, especially in the context of multiple co-morbidity and polypharmacy.


METHODS
A database linkage was established between the regional drug dispensing registry of Madrid and the Liverpool HIV DDI database (January-June 2017). Polypharmacy was defined as the use of ≥5 non-HIV medications, and DDIs were classified by a traffic-light ranking for severity. HIV-uninfected controls were also included.


RESULTS
A total of 22,945 patients living with HIV (PLWH) and 6,613,506 uninfected individuals had received medications. Antiretroviral therapy regimens were predominantly based on integrase inhibitors (51.96%). Polypharmacy was significantly higher in PLWH (32.94%) than uninfected individuals (22.16%; P<0.001), and this difference was consistently observed across all age strata except for individuals aged ≥75 years. Polypharmacy was more common in women than men in both PLWH and uninfected individuals. The prevalence of contraindicated combinations involving ARVs was 3.18%. Comedications containing corticosteroids, quetiapine, or antithrombotic agents were associated with the highest risk for red-flag DDI, and the use of raltegravir or dolutegravir-based antiretroviral therapy was associated with an adjusted odds ratio of 0.72 (95% confidence interval: 0.60 - 0.88; P=0.001) for red-flag DDI.


CONCLUSIONS
Polypharmacy was more frequent among PLWH across all age groups except those aged ≥75 years and was more common in women. The persistent detection of contraindicated medications in patients receiving ARVs suggests a likely disconnect between hospital and community prescriptions. Switching to alternative unboosted integrase regimens should be considered for patients with high risk of harm from DDIs.",2020,2020-07-11,"1413811042,1413809069,1742274491,82831565,4627386,4795019,7862074,1401579579,46822107,4532955,1413810436,145564963,1417618813,144489853","B. López-Centeno,Carlos Badenes-Olmedo,Á. Mataix-Sanjuan,K. McAllister,J. Bellón,S. Gibbons,P. Balsalobre,L. Pérez-Latorre,J. Benedí,C. Marzolini,Ainhoa Aranguren-Oyarzábal,S. Khoo,M. J. Calvo-Alcántara,J. Berenguer"
47e618a27c3d43318038c71f8928c630b1be1a73,Data Quality Barriers for Transparency in Public Procurement,"Governments need to be accountable and transparent for their public spending decisions in order to prevent losses through fraud and corruption as well as to build healthy and sustainable economies. Open data act as a major instrument in this respect by enabling public administrations, service providers, data journalists, transparency activists, and regular citizens to identify fraud or uncompetitive markets through connecting related, heterogeneous, and originally unconnected data sources. To this end, in this article, we present our experience in the case of Slovenia, where we successfully applied a number of anomaly detection techniques over a set of open disparate data sets integrated into a Knowledge Graph, including procurement, company, and spending data, through a linked data-based platform called TheyBuyForYou. We then report a set of guidelines for publishing high quality procurement data for better procurement analytics, since our experience has shown us that there are significant shortcomings in the quality of data being published. This article contributes to enhanced policy making by guiding public administrations at local, regional, and national levels on how to improve the way they publish and use procurement-related data; developing technologies and solutions that buyers in the public and private sectors can use and adapt to become more transparent, make markets more competitive, and reduce waste and fraud; and providing a Knowledge Graph, which is a data resource that is designed to facilitate integration across multiple data silos by showing how it adds context and domain knowledge to machine-learning-based procurement analytics.",2022,2022-02-20,"153750193,70053552,3170752,2134990618,2155715210,101029936,2007932882,2155714185,52208399,31951006,2927032,143759957,66215591","A. Soylu,Óscar Corcho,B. Elvesæter,Carlos Badenes-Olmedo,Francisco Yedro-Martínez,Matej Kovacic,Matej Posinkovic,Mitja Medvešček,Ian Makgill,C. Taggart,E. Simperl,T. C. Lech,D. Roman"
4f2133f97f0563f17bb24ce9ab128acfa58537e4,Challenges for FAIR Digital Object Assessment,"A Digital Object (DO) ""is a sequence of bits, incorporating a work or portion of a work or other information in which a party has rights or interests, or in which there is value"". DOs should have persistent identifiers, meta-data and be readable by both humans and machines. A FAIR Digital Object is a DO able to interact with automated data processing systems (De Smedt et al. 2020) while following the FAIR (Findable, Accessible, Interoperable and Reusable principles) principles (Wilkinson et al. 2016).
 Although FAIR was originally targeted towards data artifacts, new initiatives have emerged to adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht et al. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and even DOs (Collins et al. 2018). In this paper, we describe the challenges when assessing the level of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that a DO contains multiple resources and captures their relationships. We explore different methods to calculate an evaluation score, and we discuss the challeneges and importance of providing explanations and guidelines for authors.
 
 FAIR assessment tools 
 
 There are a growing number of tools used to assess the FAIRness of DOs. Community groups like FAIRassist.org have compiled lists of guidelines and tools for assessing the FAIRness of digital resources. These range from self assessment tools like questionnaires and checklists to semi-automated validators (Devaraju et al. 2021). Examples of automated validation tools include the F-UJI Automated FAIR Data Assessment Tool (Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individual DOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al. 2021) to assess ontologies.
 When it comes to assessing FDOs, we find two main challenges:
 
 
 
 Resource score discrepancy: Different FAIR assessment tools for the same type of resource produce different scores. For example, a recent study over datasets showcases differences in scores for the same resource due to how the FAIR principles are interpreted by different authors (Dumontier 2022).
 
 
 Heterogeneous FDO metadata: Validators include tests that explore metadata of the digital object. However, there is no agreed metadata schema to represent FDO metadata, which complicates this operation. In addition, metadata may be specific to a certain domain (De Smedt et al. 2020). To address this challenge, we need i) to agree on minimum common set of metadata to measure the FAIRness of DOs and ii) propose a framework to describe extensions for specialized digital objects (datasets, software, ontologies, VRE, etc.).
 
 
 
 Resource score discrepancy: Different FAIR assessment tools for the same type of resource produce different scores. For example, a recent study over datasets showcases differences in scores for the same resource due to how the FAIR principles are interpreted by different authors (Dumontier 2022).
 Heterogeneous FDO metadata: Validators include tests that explore metadata of the digital object. However, there is no agreed metadata schema to represent FDO metadata, which complicates this operation. In addition, metadata may be specific to a certain domain (De Smedt et al. 2020). To address this challenge, we need i) to agree on minimum common set of metadata to measure the FAIRness of DOs and ii) propose a framework to describe extensions for specialized digital objects (datasets, software, ontologies, VRE, etc.).
 In (Wilkinson et al. 2019), the authors propose a community-driven framework to assess the FAIRness of individual digital objects. This framework is based on:
 
 
 
 a collection of maturity indicators,
 
 
 principle compliance tests, and
 
 
 a module to apply those tests to digital resources.
 
 
 
 a collection of maturity indicators,
 principle compliance tests, and
 a module to apply those tests to digital resources.
 The proposed indicators may be a starting point to define which tests are needed for each type of resource (de Miranda Azevedo and Dumontier 2020).
 
 Aggregation of FAIR metrics
 
 Another challenge is the best way to produce an assessment score for a FDO, independently of the tests that are run to assess it. For example, each of the four dimensions of FAIR (Findable, Accessible, Interoperable and Reusable) usually have a different number of associated assessment tests. If the final score is presented based on the number of tests, then by default some dimensions may have more importance than others. Similarly, not all tests may have the same importance for some specific resources (e.g., in some cases having a license in a resource may be considered more important than having its full documentation).
 In our work we consider a FDO as an aggregation of resources, and therefore we face the additional challenge of creating an aggregated FAIRness score for the whole FDO. We consider the following aggregation scores:
 
 
 
 Global score: calculated by formula (see Fig. 1-1). It represents the percentage of total passed tests. It doesn’t take into account the principle to which a test belongs.
 
 
 FAIR average score: calculated by formula (see Fig. 1- 2). It represents the average of the passed tests ratios for each principle plus the ratio of passed tests used to evaluate the Research Object itself.
 
 
 
 Global score: calculated by formula (see Fig. 1-1). It represents the percentage of total passed tests. It doesn’t take into account the principle to which a test belongs.
 FAIR average score: calculated by formula (see Fig. 1- 2). It represents the average of the passed tests ratios for each principle plus the ratio of passed tests used to evaluate the Research Object itself.
 Both metrics are agnostic to the kind of resource analyzed. The score they produce ranges from [0 - 100].
 
 Discussion
 
 A FDO has metadata records that describe it. Some records are common for all DOs, and others are specific to a DO. This makes it difficult to assess some FAIR principles like ""F2: ""data are described with rich metadata"". Therefore, we believe a discussion of a minimal set of FAIR metadata should be addressed by the community.
 In addition, a FAIR assessment score can change significantly depending on the formula used for aggregating all metrics. Therefore, it is key to explain to users the method and provenance used to produce such score. Different communities should agree on the best scoring mechanism for their FDOs, e.g., by adding a weight to each principle and figuring out the right number of tests for each principle, which may give more importance to the principles with tests.
 We believe that the objective of a FAIR scoring system should not be to produce a ranking, but become a mechanism to improve the FAIRness of a FDO.",2022,2022-10-12,"2112433218,1398926410,70053552","E. González,D. Garijo,Óscar Corcho"
680c921c39e0d238777fa604e7921ba8e5b3c372,SPARQL2Flink: Evaluation of SPARQL Queries on Apache Flink,"Existing SPARQL query engines and triple stores are continuously improved to handle more massive datasets. Several approaches have been developed in this context proposing the storage and querying of RDF data in a distributed fashion, mainly using the MapReduce Programming Model and Hadoop-based ecosystems. New trends in Big Data technologies have also emerged (e.g., Apache Spark, Apache Flink); they use distributed in-memory processing and promise to deliver higher data processing performance. In this paper, we present a formal interpretation of some PACT transformations implemented in the Apache Flink DataSet API. We use this formalization to provide a mapping to translate a SPARQL query to a Flink program. The mapping was implemented in a prototype used to determine the correctness and performance of the solution. The source code of the project is available in Github under the MIT license.",2021,2021-07-30,"2132710614,2132711925,1853590,2135942475,70053552","Oscar Ceballos,Carlos Alberto Ramírez Restrepo,M. Pabón,Andres M. Castillo,Óscar Corcho"
68675db08311f1dbef1138d74d338a667aad2e24,Less Lipoatrophy and Better Lipid Profile With Abacavir as Compared to Stavudine: 96-Week Results of a Randomized Study,"Objective:To assess lipoatrophy, other toxicities, and efficacy associated with abacavir as compared with stavudine in HIV-infected antiretroviral-naive patients. Methods:This was a prospective, randomized, open trial, stratified by viral load and CD4 cell count, conducted January 2001 to July 2004. Two hundred thirty-seven adult patients with HIV infection initiating antiretroviral therapy were assigned to receive abacavir (n = 115) or stavudine (n = 122), both combined with lamivudine and efavirenz. The primary endpoint was the proportion of patients with lipoatrophy as assessed by physician and patient observation at 96 weeks. Results:A lower proportion of patients assigned to abacavir developed clinical signs of lipoatrophy (4.8% vs. 38.3%; P < 0.001). These observations were confirmed by anthropometric data. Dual energy x-ray absorptiometry (DEXA) scans performed in 57 patients showed significantly greater total limb fat loss in the stavudine arm (−1579 vs. 913 g; P < 0.001). The lipid profile in abacavir patients presented more favorable changes in the levels of triglycerides (P = 0.03), high-density lipoprotein cholesterol (HDLc; P < 0.001), and apolipoprotein A1 (P < 0.001) as well as in the ratio between total cholesterol and HDLc (P = 0.005). Throughout the study, a higher proportion of patients in the stavudine group received lipid-lowering agents as compared to the abacavir group (17% vs. 4%; P = 0.002). Similar virologic and immunologic responses were observed. Conclusions:Assuming the limitations inherent to clinical assessment, this study shows a notably weaker association of abacavir with lipoatrophy than stavudine. DEXA scans and anthropometric measurements supported the clinical findings. In addition, the lipid changes that occurred were more favorable in patients receiving abacavir.",2007,2007-02-01,"4538630,39058283,153283271,11423701,144353441,13145067,3718341,144189316,5062063,5727965,145804103,2434187,39087234,87777369,152174325,1413932406,2055962679,143699494,51996599,144489853,15193629,46615427,13354363,144550587,119911607","D. Podzamczer,E. Ferrer,P. Sanchez,J. Gatell,M. Crespo,C. Fisac,M. Loncá,J. Sanz,J. Niubó,S. Veloso,J. Llibre,P. Barrufet,M. A. Ribas,E. Merino,E. Ribera,J. Martínez-Lacasa,C. Alonso,M. Aranda,F. Pulido,J. Berenguer,A. Delegido,J. D. Pedreira,A. Lérida,R. Rubio,L. D. Rio"
6d702da5dea94d0b0a9302ae579ce995357dd960,An ontological approach for representing declarative mapping languages,"Knowledge Graphs are currently created using an assortment of techniques and tools: ad hoc code in a programming language, database export scripts, OpenRefine transformations, mapping languages, etc. Focusing on the latter, the wide variety of use cases, data peculiarities, and potential uses has had a substantial impact in how mappings have been created, extended, and applied. As a result, a large number of languages and their associated tools have been created. In this paper, we present the Conceptual Mapping ontology, that is designed to represent the features and characteristics of existing declarative mapping languages to construct Knowledge Graphs. This ontology is built upon the requirements extracted from experts experience, a thorough analysis of the features and capabilities of current mapping languages presented as a comparative framework; and the languages’ limitations discussed by the community and denoted as Mapping Challenges. The ontology is evaluated to ensure that it meets these requirements and has no inconsistencies, pitfalls or modelling errors, and is publicly available online along with its documentation and related resources.",2022,2022-12-29,"1485497099,3413290,1792630,1404333852,1398854351,70053552","Ana Iglesias-Molina,Andrea Cimmino,E. Ruckhaus,David Chaves-Fraga,R. García-Castro,Óscar Corcho"
7737b2f4f956e2517087a4be04cd9e1af70961ea,Large-scale semantic exploration of scientific literature using topic-based hashing algorithms,"Searching for similar documents and exploring major themes covered across groups of documents are common activities when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead to unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms represent documents in a common feature space that abstract them away from the specific sequence of words used in them. Probabilistic Topic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latent space some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematic information gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics to justify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extending those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations on both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.",2020,,"1413809069,1403988745,70053552","Carlos Badenes-Olmedo,J. Redondo-García,Óscar Corcho"
7ad45285b98b20661ab23d5a43dfe8783fafc02c,Impact of Text Length for Information Retrieval Tasks based on Probabilistic Topics,"This work is supported by the project KnowledgeSpaces with reference PID2020-118274RB-I00, financed by the Spanish Ministry of Science and Innovation.",2021,2021-09-06,"2134990618,2134990341,70053552","Carlos Badenes-Olmedo,Borja Lozano-Álvarez,Óscar Corcho"
7f085d4e71c713d9db7a3d32c302e1fe58129733,A High-Level Ontology Network for ICT Infrastructures,,2021,,"70053552,1404333852,2056266963,2107052011,1413809069,2115270260,2140198525,2130220243,2059607208,2134108404","Óscar Corcho,David Chaves-Fraga,Jhon Toledo,Julián Arenas-Guerrero,Carlos Badenes-Olmedo,Mingxue Wang,Hu Peng,Nicholas Burrett,José Mora,Puchao Zhang"
80313ba1f12e4525b941ba29f8e020cf5ae8b835,"An Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus","Several initiatives have emerged during the COVID-19 pandemic to gather scientific publications related to coronaviruses. Among them, the COVID-19 Open Research Dataset (CORD-19) has proven to be a valuable resource that provides full-text articles from the PubMed Central, bioRxiv and medRxiv repositories. Such a large amount of biomedical literature needs to be properly managed to facilitate and promote its use by health professionals, for example by tagging documents with the biomedical entities that appear on them. We created a biomedical named entity recognizer (NER) that normalizes (NEN) the drugs, diseases, genes and proteins mentioned in texts with the codes of the main standardization systems such as MeSH, ICD-10, ATC, SNOMED, ChEBI, GARD and NCBI. It is based on fine-tuning the BioBERT language model independently for each entity type using domain-specific datasets and an inverse index search to normalize the references. We have used the resultant BioNER+BioNEN system to process the CORD-19 corpus and offer an overview of the drugs, diseases, genes and proteins related to coronaviruses in the last fifty years.",2022,,"2134990618,2072861538,70053552","Carlos Badenes-Olmedo,Álvaro Alonso,Óscar Corcho"
87b1e3c5f485efc52c8cdcf20fa0b63346e6d181,A Semantic Sensor Web for Environmental Decision Support Applications,"Sensing devices are increasingly being deployed to monitor the physical world around us. One class of application for which sensor data is pertinent is environmental decision support systems, e.g., flood emergency response. For these applications, the sensor readings need to be put in context by integrating them with other sources of data about the surrounding environment. Traditional systems for predicting and detecting floods rely on methods that need significant human resources. In this paper we describe a semantic sensor web architecture for integrating multiple heterogeneous datasets, including live and historic sensor data, databases, and map layers. The architecture provides mechanisms for discovering datasets, defining integrated views over them, continuously receiving data in real-time, and visualising on screen and interacting with the data. Our approach makes extensive use of web service standards for querying and accessing data, and semantic technologies to discover and integrate datasets. We demonstrate the use of our semantic sensor web architecture in the context of a flood response planning web application that uses data from sensor networks monitoring the sea-state around the coast of England.",2011,2011-09-14,"34071005,144030350,144121902,1792820,1943226,1795889,49193745,1398854351,34879237,2445257,144779687,1716510,70053552,1746733,1708123,144685851,1398348796","A. Gray,J. Sadler,O. Kit,K. Kyzirakos,M. Karpathiotakis,Jean-Paul Calbimonte,Kevin R. Page,R. García-Castro,A. Frazer,I. Galpin,A. Fernandes,N. Paton,Óscar Corcho,Manolis Koubarakis,D. D. Roure,K. Martinez,A. Gómez-Pérez"
99719940773bce8b802491cf1f2a11339e13c4d2,Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies,"With the ongoing growth in number of digital articles in a wider set of languages and the expanding use of different languages, we need annotation methods that enable browsing multi-lingual corpora. Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, these approaches require theme-aligned training data to create a language-independent space. This constraint limits the amount of scenarios that this technique can offer solutions to train and makes it difficult to scale up to situations where a huge collection of multi-lingual documents are required during the training phase. This paper presents an unsupervised document similarity algorithm that does not require parallel or comparable corpora, or any other type of translation resource. The algorithm annotates topics automatically created from documents in a single language with cross-lingual labels and describes documents by hierarchies of multi-lingual concepts from independently-trained models. Experiments performed on the English, Spanish and French editions of JCR-Acquis corpora reveal promising results on classifying and sorting documents by similar content.",2019,2019-09-23,"1413809069,1787607,70053552","Carlos Badenes-Olmedo,José Luis Redondo García,Óscar Corcho"
a91064924edc6398c1aa266948bf1f515261fc6f,TheyBuyForYou platform and knowledge graph: Expanding horizons in public procurement with open linked data,"Public procurement is a large market affecting almost every organisation and individual; therefore, governments need to ensure its efficiency, transparency, and accountability, while creating healthy, competitive, and vibrant economies. In this context, open data initiatives and integration of data from multiple sources across national borders could transform the procurement market by such as lowering the barriers of entry for smaller suppliers and encouraging healthier competition, in particular by enabling cross-border bids. Increasingly more open data is published in the public sector; however, these are created and maintained in siloes and are not straightforward to reuse or maintain because of technical heterogeneity, lack of quality, insufficient metadata, or missing links to related domains. To this end, we developed an open linked data platform, called TheyBuyForYou, consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border, and cross-lingual procurement knowledge graph. We developed advanced tools and services on top of the knowledge graph for anomaly detection, cross-lingual document search, and data storytelling. This article describes the TheyBuyForYou platform and knowledge graph, reports their adoption by different stakeholders and challenges and experiences we went through while creating them, and demonstrates the usefulness of Semantic Web and Linked Data technologies for enhancing public procurement.",2021,2021-09-03,"153750193,70053552,3170752,2134990618,2191156,2135341749,101029936,2007932882,52208399,31951006,2927032,143759957,66215591","A. Soylu,Óscar Corcho,B. Elvesæter,Carlos Badenes-Olmedo,Tom Blount,Francisco Yedro Martínez,Matej Kovacic,Matej Posinkovic,Ian Makgill,C. Taggart,E. Simperl,T. C. Lech,D. Roman"
a98c96d461b767ff3d87afc4bcb0c520484d281e,Enabling Query Technologies for the Semantic Sensor Web,"Sensor networks are increasingly being deployed in the environment for many different purposes. The observations that they produce are made available with heterogeneous schemas, vocabularies and data formats, making it difficult to share and reuse this data, for other purposes than those for which they were originally set up. The authors propose an ontology-based approach for providing data access and query capabilities to streaming data sources, allowing users to express their needs at a conceptual level, independent of implementation and language-specific details. In this article, the authors describe the theoretical foundations and technologies that enable exposing semantically enriched sensor metadata, and querying sensor observations through SPARQL extensions, using query rewriting and data translation techniques according to mapping languages, and managing both pull and push delivery modes.",2012,,"1795889,34436207,70053552,1751802","Jean-Paul Calbimonte,Hoyoung Jeung,Óscar Corcho,K. Aberer"
bedfb656e33029932a94e0b4ca40b6b5dd47af26,Potentially inappropriate medications in older adults living with HIV.,"OBJECTIVES
We assessed the prevalence of potentially inappropriate medication (PIM) among older (≥ 65 years) people living with HIV (O-PLWH) in the region of Madrid.


METHODS
We analysed the dispensation registry of community and hospital pharmacies from the Madrid Regional Health Service (SERMAS) for the period between 1 January and 30 June 2017, looking specifically at PIMs according to the 2019 Beers criteria. Co-medications were classified according to the Anatomical Therapeutic Chemical (ATC) classification system.


RESULTS
A total of 6 636 451 individuals received medications. Of these individuals, 22 945 received antiretrovirals (ARVs), and of these 1292 were O-PLWH. Overall, 1135 (87.8%) O-PLWH were taking at least one co-medication, and polypharmacy (at least five co-medications) was observed in 852 individuals (65.9%). A PIM was identified in 482 (37.3%) O-PLWH. Factors independently associated with PIM were polypharmacy [adjusted odds ratio (aOR) 7.08; 95% confidence interval (CI) 5.16-9.72] and female sex (aOR 1.75; 95% CI 1.30-2.35). The distribution of PIMs according to ATC drug class were nervous system drugs (n = 369; 28.6%), musculoskeletal system drugs (n = 140; 10.8%), gastrointestinal and metabolism drugs (n = 72; 5.6%), cardiovascular drugs (n = 61; 4.7%), respiratory system drugs (n = 13; 1.0%), antineoplastic and immunomodulating drugs (n = 10; 0.8%), and systemic anti-infectives (n = 2; 0.2%). Five drugs accounted for 84.8% of the 482O PLWH with PIMs: lorazepam (38.2%), ibuprofen (18.0%), diazepam (10.2%), metoclopramide (9.9%), and zolpidem (8.5%).


CONCLUSIONS
Prescription of PIMs is highly prevalent in O-PLWH. Consistent with data in uninfected elderly people, the most frequently observed PIMs were benzodiazepines and nonsteroidal anti-inflammatory drugs . Targeted interventions are warranted to reduce inappropriate prescribing and polypharmacy in this vulnerable population.",2020,2020-06-09,"1413811042,1413809069,1742274491,4627386,1401579579,1963241010,46822107,145564965,4532955,1417618813,144489853","B. López-Centeno,Carlos Badenes-Olmedo,Á. Mataix-Sanjuan,J. Bellón,L. Pérez-Latorre,J. C. López,J. Benedí,S. Khoo,C. Marzolini,M. J. Calvo-Alcántara,J. Berenguer"
befeaee41925b4ece2b198b5419f50274e37a629,Five challenges for the Semantic Sensor Web,"The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet. Most efforts in this area focused on the provision of platforms that could be used to build sensor-based applications more efficiently, considering some of the most important challenges in sensor-based data management and sensor network configuration. The introduction of semantics into these platforms provides the opportunity of going a step forward into the understanding, management and use of sensor-based data sources, and this is a topic being explored by ongoing initiatives. In this paper we go through some of the most relevant challenges of the current Sensor Web, and describe some ongoing work and open opportunities for the introduction of semantics in this context.",2010,2010-04-01,"70053552,1398854351","Óscar Corcho,R. García-Castro"
c5cb8b18702817fe20387d3c0be82e292577b976,EBOCA: Evidences for BiOmedical Concepts Association Ontology,". There is a large number of online documents data sources available nowadays. The lack of structure and the diﬀerences between formats are the main diﬃculties to automatically extract information from them, which also has a negative impact on its use and reuse. In the biomedical domain, the DISNET platform emerged to provide re-searchers with a resource to obtain information in the scope of human disease networks by means of large-scale heterogeneous sources. Specif-ically in this domain, it is critical to oﬀer not only the information extracted from diﬀerent sources, but also the evidence that supports it. This paper proposes EBOCA, an ontology that describes (i) biomedical domain concepts and associations between them, and (ii) evidences supporting these associations; with the objective of providing an schema to improve the publication and description of evidences and biomedical associations in this domain. The ontology has been successfully evaluated to ensure there are no errors, modelling pitfalls and that it meets the previously deﬁned functional requirements. Test data coming from a subset of DISNET and automatic association extractions from texts has been transformed according to the proposed ontology to create a Knowledge Graph that can be used in real scenarios, and which has also been used for the evaluation of the presented ontology.",2022,2022-08-01,"2180133273,1485497099,2180025176,2065739771,1413809069,2180027960","Andrea 'Alvarez P'erez,Ana Iglesias-Molina,Luc'ia Prieto Santamar'ia,Mar'ia Poveda-Villal'on,Carlos Badenes-Olmedo,Alejandro Rodr'iguez-Gonz'alez"
c5f0195d35bb1b56fbbbdfff50de250955e929fe,Formalisation and experiences of R2RML-based SPARQL to SQL query translation using morph,"R2RML is used to specify transformations of data available in relational databases into materialised or virtual RDF datasets. SPARQL queries evaluated against virtual datasets are translated into SQL queries according to the R2RML mappings, so that they can be evaluated over the underlying relational database engines. In this paper we describe an extension of a well-known algorithm for SPARQL to SQL translation, originally formalised for RDBMS-backed triple stores, that takes into account R2RML mappings. We present the result of our implementation using queries from a synthetic benchmark and from three real use cases, and show that SPARQL queries can be in general evaluated as fast as the SQL queries that would have been generated by SQL experts if no R2RML mappings had been used.",2014,2014-04-07,"2787942,70053552,1703204","Freddy Priyatna,Óscar Corcho,Juan Sequeda"
c79d2d38ef2ab8e7170792171a0e3b7ffb9aa886,Transforming meteorological data into Linked Data,"We describe the AEMET meteorological dataset, which makes available some data sources from the Agencia Estatal de Meteorologia AEMET, Spanish Meteorological Office as Linked Data. The data selected for publication are generated every ten minutes by approximately 250 automatic weather stations deployed across Spain and made available as CSV files in the AEMET FTP server. These files are retrieved from the server, processed with Python scripts, transformed to RDF according to an ontology network which reuses the W3C SSN Ontology, published in a triple store and visualized using Map4RDF.",2012,2012-07-01,"1750062,70053552,1398926410,2059607208,1403446235,2096825607,1403849795,1398348791","G. Atemezing,Óscar Corcho,D. Garijo,José Mora,M. Poveda-Villalón,Pablo Rozas,Daniel Vila-Suero,B. Villazón-Terrazas"
d2685ecfb612493b44b6eaecb5533e8da7b4b7c3,Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature,,2023,2023-05-01,"2134990618,70053552","Carlos Badenes-Olmedo,Óscar Corcho"
f6547db9118871c0fa9e503375afebaa27983d86,Handling qualitative preferences in SPARQL over virtual ontology-based data access,"With the increase of data volume in heterogeneous datasets that are being published following Open Data initiatives, new operators are necessary to help users to find the subset of data that best satisfies their preference criteria. Quantitative approaches such as top-k queries may not be the most appropriate approaches as they require the user to assign weights that may not be known beforehand to a scoring function. Unlike the quantitative approach, under the qualitative approach, which includes the well-known skyline, preference criteria are more intuitive in certain cases and can be expressed more naturally. In this paper, we address the problem of evaluating SPARQL qualitative preference queries over an Ontology-Based Data Access (OBDA) approach, which provides uniform access over multiple and heterogeneous data sources. Our main contribution is Morph-Skyline++, a framework for processing SPARQL qualitative preferences by directly querying relational databases. Our framework implements a technique that translates SPARQL qualitative preference queries directly into queries that can be evaluated by a relational database management system. We evaluate our approach over different scenarios, reporting the effects of data distribution, data size, and query complexity on the performance of our proposed technique in comparison with state-of-the-art techniques. Obtained results suggest that the execution time can be reduced by up to two orders of magnitude in comparison to current techniques scaling up to larger datasets while identifying precisely the result set.",2022,2022-01-13,"145182016,1404333852,70053552","Marlene Goncalves,David Chaves-Fraga,Óscar Corcho"
