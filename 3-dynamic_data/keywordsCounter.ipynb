{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnull\n",
    "import shutil\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Crear la sesión Spark\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"keywordCount\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "# Preprocesado\n",
    "# =====================================================================\n",
    "# Leer todos los archivos JSON del corpus (uniéndolos en un mismo DataFrame)\n",
    "directory = './datain/'\n",
    "# Lista de archivos en el directorio\n"
    "corpus = os.listdir(directory)\n"
    "\n",
    "json_df = spark.read.json(["datain/" + file for file in corpus])\n"
    "\n",
    "# Eliminar registros con abstract nulo\n",
    "json_df_filtered = json_df.filter(col('abstract').isNotNull())\n",
    "\n",
    "# MAP-REDUCE\n",
    "# =====================================================================\n",
    "# Seleccionar el campo abstract y convertirlo en un RDD\n",
    "words_rdd = json_df_filtered.select(\"abstract\").rdd.flatMap(lambda x: x[0].split())\n",
    "\n",
    "# Lista de palabras\n",
    "words_to_count = [\"Science\", \"artificial\", \"intelligence\", \"The\"]\n",
    "words_to_count = [word.lower() for word in words_to_count]\n",
    "\n",
    "# Contar las palabras\n",
    "counts_rdd = words_rdd.filter(lambda word: word.lower() in words_to_count).map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(counts_rdd.collect())\n",
    "\n",
    "# Keywords.csv\n",
    "# =====================================================================\n",
    "# Guardar en CSV\n",
    "counts_df = counts_rdd.toDF() # convertir a DataFrame la lista resultado\n",
    "counts_df.write.csv(\"dataout/temp\", mode=\"overwrite\") # guardamos los resultados de cada worker\n",
    "\n",
    "# Obtener todos los archivos CSV generados en el directorio temporal\n",
    "csv_files = glob.glob(\"dataout/temp/*.csv\")\n",
    "\n",
    "# Fusionar los archivos CSV en uno solo\n",
    "with open(\"dataout/Keywords.csv\", \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"word\", \"frequency\"]) # Escribir los nuevos nombres de las columnas\n",
    "\n",
    "    # Leer cada archivo CSV y copiar sus contenidos al archivo de salida\n",
    "    for csv_file in csv_files:\n",
    "        with open(csv_file, \"r\", newline=\"\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "                \n",
    "# Borramos el fichero temp\n",
    "folder_path = \"dataout/temp\"\n",
    "shutil.rmtree(folder_path) # borramos la carpeta y su contenido\n",
    "\n",
    "# Cerrar la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
